<!--
This HTML is auto-generated from an m-file.
Your changes will be overwritten.
--><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:x-large">Linearly Dependent Problem</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">A linear neuron is trained to find the minimum error solution for a problem
with linearly dependent input vectors.  If a linear dependence in input
vectors is not matched in the target vectors, the problem is nonlinear and
does not have a zero error linear solution.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Copyright 1992-2002 The MathWorks, Inc.
$Revision: 1.12 $  $Date: 2002/03/29 19:36:15 $
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">P defines three 2-element input patterns (column vectors).  Note that 0.5
times the sum of (column) vectors 1 and 3 results in vector 2.  This is called
linear dependence.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">P = [ 1.0   2.0   3.0; <span style="color:blue">...</span>
      4.0   5.0   6.0];</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">T defines an associated 1-element target (column vectors).  Note that 0.5
times the sum of -1.0 and 0.5 does not equal 1.0.  Because the linear
dependence in P is not matched in T this problem is nonlinear and does not
have a zero error linear solution.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">T = [0.5 1.0 -1.0];</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">MAXLINLR finds the fastest stable learning rate for TRAINWH.  NEWLIN creates a
linear neuron.  NEWLIN takes these arguments: 1) Rx2 matrix of min and max
values for R input elements, 2) Number of elements in the output vector, 3)
Input delay vector, and 4) Learning rate.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">maxlr = maxlinlr(P,<span style="color:#B20000">'bias'</span>);
net = newlin([0 10;0 10],1,[0],maxlr);</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">TRAIN uses the Widrow-Hoff rule to train linear networks by default.  We will
display each 50 epochs and train for a maximum of 500 epochs.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">net.trainParam.show = 50;     <span style="color:green">% Frequency of progress displays (in epochs).</span>
net.trainParam.epochs = 500;  <span style="color:green">% Maximum number of epochs to train.</span>
net.trainParam.goal = 0.001;  <span style="color:green">% Sum-squared error goal.</span></pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Now the network is trained on the inputs P and targets T.  Note that, due to
the linear dependence between input vectors, the problem did not reach the
error goal represented by the black line.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">[net,tr] = train(net,P,T);</pre><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:gray; font-style:italic;">TRAINB, Epoch 0/500, MSE 0.75/0.001.
TRAINB, Epoch 50/500, MSE 0.53429/0.001.
TRAINB, Epoch 100/500, MSE 0.434609/0.001.
TRAINB, Epoch 150/500, MSE 0.388044/0.001.
TRAINB, Epoch 200/500, MSE 0.366292/0.001.
TRAINB, Epoch 250/500, MSE 0.35613/0.001.
TRAINB, Epoch 300/500, MSE 0.351384/0.001.
TRAINB, Epoch 350/500, MSE 0.349166/0.001.
TRAINB, Epoch 400/500, MSE 0.34813/0.001.
TRAINB, Epoch 450/500, MSE 0.347646/0.001.
TRAINB, Epoch 500/500, MSE 0.34742/0.001.
TRAINB, Maximum epoch reached.

</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demolin6_img06.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">We can now test the associator with one of the original inputs, [1; 4] , and
see if it returns the target, 0.5.  The result is not 0.5 as the linear
network could not fit the nonlinear problem caused by the linear dependence
between input vectors.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">p = [1.0; 4];
a = sim(net,p)</pre><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:gray; font-style:italic;">
a =

    0.8971

</pre><originalCode xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" code="%% Linearly Dependent Problem&#xA;% A linear neuron is trained to find the minimum error solution for a problem&#xA;% with linearly dependent input vectors.  If a linear dependence in input&#xA;% vectors is not matched in the target vectors, the problem is nonlinear and&#xA;% does not have a zero error linear solution.&#xA;%&#xA;% Copyright 1992-2002 The MathWorks, Inc.&#xA;% $Revision: 1.12 $  $Date: 2002/03/29 19:36:15 $&#xA;&#xA;%%&#xA;% P defines three 2-element input patterns (column vectors).  Note that 0.5&#xA;% times the sum of (column) vectors 1 and 3 results in vector 2.  This is called&#xA;% linear dependence.&#xA;&#xA;P = [ 1.0   2.0   3.0; ...&#xA;      4.0   5.0   6.0];&#xA;&#xA;%%&#xA;% T defines an associated 1-element target (column vectors).  Note that 0.5&#xA;% times the sum of -1.0 and 0.5 does not equal 1.0.  Because the linear&#xA;% dependence in P is not matched in T this problem is nonlinear and does not&#xA;% have a zero error linear solution.&#xA;&#xA;T = [0.5 1.0 -1.0];&#xA;&#xA;%%&#xA;% MAXLINLR finds the fastest stable learning rate for TRAINWH.  NEWLIN creates a&#xA;% linear neuron.  NEWLIN takes these arguments: 1) Rx2 matrix of min and max&#xA;% values for R input elements, 2) Number of elements in the output vector, 3)&#xA;% Input delay vector, and 4) Learning rate.&#xA;&#xA;maxlr = maxlinlr(P,'bias');&#xA;net = newlin([0 10;0 10],1,[0],maxlr);&#xA;&#xA;%%&#xA;% TRAIN uses the Widrow-Hoff rule to train linear networks by default.  We will&#xA;% display each 50 epochs and train for a maximum of 500 epochs.&#xA;&#xA;net.trainParam.show = 50;     % Frequency of progress displays (in epochs).&#xA;net.trainParam.epochs = 500;  % Maximum number of epochs to train.&#xA;net.trainParam.goal = 0.001;  % Sum-squared error goal.&#xA;&#xA;%%&#xA;% Now the network is trained on the inputs P and targets T.  Note that, due to&#xA;% the linear dependence between input vectors, the problem did not reach the&#xA;% error goal represented by the black line.&#xA;&#xA;[net,tr] = train(net,P,T);&#xA;&#xA;%%&#xA;% We can now test the associator with one of the original inputs, [1; 4] , and&#xA;% see if it returns the target, 0.5.  The result is not 0.5 as the linear&#xA;% network could not fit the nonlinear problem caused by the linear dependence&#xA;% between input vectors.&#xA;&#xA;p = [1.0; 4];&#xA;a = sim(net,p)&#xA;"></originalCode>