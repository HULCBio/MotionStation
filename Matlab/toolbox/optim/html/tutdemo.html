<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Tutorial for the Optimization Toolbox.</title>
      <meta name="generator" content="MATLAB 7.0">
      <meta name="date" content="2004-04-17">
      <meta name="m-file" content="tutdemo"><style>
body {
  background-color: white;
  margin:10px;
}
h1 {
  color: #990000; 
  font-size: x-large;
}
h2 {
  color: #990000;
  font-size: medium;
}
p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

pre.codeinput {
  margin-left: 30px;
}

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.showbuttons {
  margin-left: 30px;
  border: solid black 2px;
  padding: 4px;
  background: #EBEFF3;
}

pre.codeoutput {
  color: gray;
  font-style: italic;
}
pre.error {
  color: red;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows.  On Gecko-based browsers, the shrink-to-fit doesn't work. */ 
p,h1,h2,div {
  /* for MATLAB's browser */
  width: 600px;
  /* for Mozilla, but the "width" tag overrides it anyway */
  max-width: 600px;
  /* for IE */
  width:expression(document.body.clientWidth > 620 ? "600px": "auto" );
}

    </style></head>
   <body>
      <h1>Tutorial for the Optimization Toolbox.</h1>
      <introduction>
         <p>This is a demonstration for the medium-scale algorithms in the Optimization Toolbox. It closely follows the Tutorial section
            of the users' guide.
         </p>
         <p>All the principles outlined in this demonstration apply to the other nonlinear solvers: FGOALATTAIN, FMINIMAX, LSQNONLIN,
            FSOLVE.
         </p>
         <p>The routines differ from the Tutorial Section examples in the User's Guide only in that some objectives are anonymous functions
            instead of M-file functions.
         </p>
      </introduction>
      <h2>Contents</h2>
      <div>
         <ul>
            <li><a href="#1">Unconstrained optimization example</a></li>
            <li><a href="#8">Constrained optimization example: inequalities</a></li>
            <li><a href="#18">Constrained optimization example: inequalities and bounds</a></li>
            <li><a href="#29">Constrained optimization example: user-supplied gradients</a></li>
            <li><a href="#40">Constrained optimization example: equality constraints</a></li>
            <li><a href="#50">Changing the default termination tolerances</a></li>
         </ul>
      </div>
      <h2>Unconstrained optimization example<a name="1"></a></h2>
      <p>Consider initially the problem of finding a minimum of the function:</p><pre>                              2        2
     f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)</pre><p>Define the objective to be minimized as an anonymous function:</p><pre class="codeinput">fun = @(x) exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1)
</pre><pre class="codeoutput">
fun = 

    @(x) exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1)

</pre><p>Take a guess at the solution:</p><pre class="codeinput">x0 = [-1; 1];
</pre><p>Set optimization options: turn off the large-scale algorithms (the default):</p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
</pre><p>Call the unconstrained minimization function:</p><pre class="codeinput">[x, fval, exitflag, output] = fminunc(fun, x0, options);
</pre><pre class="codeoutput">Optimization terminated: relative infinity-norm of gradient less than options.TolFun.
</pre><p>The optimizer has found a solution at:</p><pre class="codeinput">x
</pre><pre class="codeoutput">
x =

    0.5000
   -1.0000

</pre><p>The function value at the solution is:</p><pre class="codeinput">fval
</pre><pre class="codeoutput">
fval =

  1.0983e-015

</pre><p>The total number of function evaluations was:</p><pre class="codeinput">output.funcCount
</pre><pre class="codeoutput">
ans =

    66

</pre><h2>Constrained optimization example: inequalities<a name="8"></a></h2>
      <p>Consider the above problem with two additional constraints:</p><pre>                                    2        2
 minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)</pre><pre> subject to  1.5 + x(1).x(2) - x(1) - x(2) &lt;= 0
                 - x(1).x(2)               &lt;= 10</pre><p>The objective function this time is contained in an M-file, objfun.m:</p><pre class="codeinput">type <span class="string">objfun</span>
</pre><pre class="codeoutput">
function f = objfun(x)
% Objective function

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.5.4.2 $  $Date: 2004/04/06 01:10:28 $

f = exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1);

</pre><p>The constraints are also defined in an M-file, confun.m:</p><pre class="codeinput">type <span class="string">confun</span>
</pre><pre class="codeoutput">
function [c, ceq] = confun(x)
% Nonlinear inequality constraints:

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.6.4.2 $  $Date: 2004/04/06 01:10:16 $

c = [1.5 + x(1)*x(2) - x(1) - x(2); 
     -x(1)*x(2) - 10];
% No nonlinear equality constraints:
ceq = [];

</pre><p>Take a guess at the solution:</p><pre class="codeinput">x0 = [-1 1];
</pre><p>Set optimization options: turn off the large-scale algorithms (the default) and turn on the display of results at each iteration:</p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>,<span class="string">'Display'</span>,<span class="string">'iter'</span>);
</pre><p>Call the optimization algorithm. We have no linear equalities or inequalities or bounds, so we pass [] for those arguments:</p><pre class="codeinput">[x,fval,exitflag,output] = fmincon(@objfun,x0,[],[],[],[],[],[],@confun,options);
</pre><pre class="codeoutput">
                               max                   Directional   First-order 
 Iter F-count        f(x)   constraint    Step-size   derivative   optimality Procedure 
    0      3       1.8394          0.5                                         Infeasible start point
    1      7      1.85127     -0.09197            1       -0.027        0.778   
    2     11     0.300167         9.33            1       -0.825        0.313  Hessian modified twice  
    3     15     0.529835       0.9209            1        0.302        0.232   
    4     20     0.186965       -1.517          0.5       -0.437         0.13   
    5     24    0.0729085       0.3313            1      -0.0715        0.054   
    6     28    0.0353323     -0.03303            1       -0.026       0.0271   
    7     32    0.0235566     0.003184            1     -0.00963      0.00587   
    8     36    0.0235504   9.032e-008            1   -6.22e-006    8.51e-007   
Optimization terminated: first-order optimality measure less
 than options.TolFun and maximum constraint violation is less
 than options.TolCon.
Active inequalities (to within options.TolCon = 1e-006):
  lower      upper     ineqlin   ineqnonlin
                                     1
                                     2
</pre><p>A solution to this problem has been found at:</p><pre class="codeinput">x
</pre><pre class="codeoutput">
x =

   -9.5474    1.0474

</pre><p>The function value at the solution is:</p><pre class="codeinput">fval
</pre><pre class="codeoutput">
fval =

    0.0236

</pre><p>Both inequality constraints are satisfied (and active) at the solution:</p><pre class="codeinput">[c, ceq] = confun(x)
</pre><pre class="codeoutput">
c =

  1.0e-007 *

   -0.9032
    0.9032


ceq =

     []

</pre><p>The total number of function evaluations was:</p><pre class="codeinput">output.funcCount
</pre><pre class="codeoutput">
ans =

    36

</pre><h2>Constrained optimization example: inequalities and bounds<a name="18"></a></h2>
      <p>Consider the previous problem with additional bound constraints:</p><pre>                                    2        2
 minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)</pre><pre> subject to  1.5 + x(1).x(2) - x(1) - x(2) &lt;= 0
                 - x(1).x(2)               &lt;= 10</pre><pre>        and                    x(1)        &gt;= 0
                                      x(2) &gt;= 0</pre><p>As in the previous example, the objective and constraint functions are defined in M-files. The file objfun.m contains the
         objective:
      </p><pre class="codeinput">type <span class="string">objfun</span>
</pre><pre class="codeoutput">
function f = objfun(x)
% Objective function

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.5.4.2 $  $Date: 2004/04/06 01:10:28 $

f = exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1);

</pre><p>The file confun.m contains the constraints:</p><pre class="codeinput">type <span class="string">confun</span>
</pre><pre class="codeoutput">
function [c, ceq] = confun(x)
% Nonlinear inequality constraints:

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.6.4.2 $  $Date: 2004/04/06 01:10:16 $

c = [1.5 + x(1)*x(2) - x(1) - x(2); 
     -x(1)*x(2) - 10];
% No nonlinear equality constraints:
ceq = [];

</pre><p>Set the bounds on the variables:</p><pre class="codeinput">lb = zeros(1,2); <span class="comment">% Lower bounds x &gt;= 0</span>
ub = [];         <span class="comment">% No upper bounds</span>
</pre><p>Again, make a guess at the solution:</p><pre class="codeinput">x0 = [-1 1];
</pre><p>Set optimization options: turn off the large-scale algorithms (the default). This time we do not turn on the Display option.</p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
</pre><p>Run the optimization algorithm:</p><pre class="codeinput">[x,fval,exitflag,output] = fmincon(@objfun,x0,[],[],[],[],lb,ub,@confun,options);
</pre><pre class="codeoutput">Optimization terminated: first-order optimality measure less
 than options.TolFun and maximum constraint violation is less
 than options.TolCon.
Active inequalities (to within options.TolCon = 1e-006):
  lower      upper     ineqlin   ineqnonlin
    1                                1
</pre><p>The solution to this problem has been found at:</p><pre class="codeinput">x
</pre><pre class="codeoutput">
x =

         0    1.5000

</pre><p>The function value at the solution is:</p><pre class="codeinput">fval
</pre><pre class="codeoutput">
fval =

    8.5000

</pre><p>The constraint values at the solution are:</p><pre class="codeinput">[c, ceq] = confun(x)
</pre><pre class="codeoutput">
c =

     0
   -10


ceq =

     []

</pre><p>The total number of function evaluations was:</p><pre class="codeinput">output.funcCount
</pre><pre class="codeoutput">
ans =

    15

</pre><h2>Constrained optimization example: user-supplied gradients<a name="29"></a></h2>
      <p>Optimization problems can be solved more efficiently and accurately if gradients are supplied by the user. This demo shows
         how this may be performed. We again solve the inequality-constrained problem
      </p><pre>                                    2        2
 minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)</pre><pre> subject to  1.5 + x(1).x(2) - x(1) - x(2) &lt;= 0
                 - x(1).x(2)               &lt;= 10</pre><p>The objective function and its gradient are defined in the M-file objfungrad.m:</p><pre class="codeinput">type <span class="string">objfungrad</span>
</pre><pre class="codeoutput">
function [f, G] = objfungrad(x)
% objective function:

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.6.4.1 $  $Date: 2004/02/07 19:13:23 $

f =exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1);
% gradient (partial derivatives) of the objective function:
t = exp(x(1))*(4*x(1)^2+2*x(2)^2+4*x(1)*x(2)+2*x(2)+1);
G = [ t + exp(x(1)) * (8*x(1) + 4*x(2)), 
      exp(x(1))*(4*x(1)+4*x(2)+2)];

</pre><p>The constraints and their partial derivatives are contained in the M-file confungrad:</p><pre class="codeinput">type <span class="string">confungrad</span>
</pre><pre class="codeoutput">
function [c, ceq, dc, dceq] = confungrad(x)
% Nonlinear inequality constraints:

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.5.4.1 $  $Date: 2004/02/07 19:12:54 $

c = [1.5 + x(1)*x(2) - x(1) - x(2); 
     -x(1)*x(2) - 10];
% Gradient (partial derivatives) of nonlinear inequality constraints:
dc = [x(2)-1, -x(2); 
      x(1)-1, -x(1)];
% no nonlinear equality constraints (and gradients)
ceq = [];
dceq = [];
</pre><p>Make a guess at the solution:</p><pre class="codeinput">x0 = [-1; 1];
</pre><p>Set optimization options: since we are supplying the gradients, we have the choice to use either the medium- or the large-scale
         algorithm; we will continue to use the same algorithm for comparison purposes.
      </p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
</pre><p>We also set options to use the gradient information in the objective and constraint functions. Note: these options MUST be
         turned on or the gradient information will be ignored.
      </p><pre class="codeinput">options = optimset(options,<span class="string">'GradObj'</span>,<span class="string">'on'</span>,<span class="string">'GradConstr'</span>,<span class="string">'on'</span>);
</pre><p>Call the optimization algorithm:</p><pre class="codeinput">[x,fval,exitflag,output] = fmincon(@objfungrad,x0,[],[],[],[],[],[], <span class="keyword">...</span>
                                   @confungrad,options);
</pre><pre class="codeoutput">Optimization terminated: first-order optimality measure less
 than options.TolFun and maximum constraint violation is less
 than options.TolCon.
Active inequalities (to within options.TolCon = 1e-006):
  lower      upper     ineqlin   ineqnonlin
                                     1
                                     2
</pre><p>As before, the solution to this problem has been found at:</p><pre class="codeinput">x
</pre><pre class="codeoutput">
x =

   -9.5474
    1.0474

</pre><p>The function value at the solution is:</p><pre class="codeinput">fval
</pre><pre class="codeoutput">
fval =

    0.0236

</pre><p>Both inequality constraints are active at the solution:</p><pre class="codeinput">[c, ceq] = confungrad(x)
</pre><pre class="codeoutput">
c =

  1.0e-007 *

   -0.9032
    0.9032


ceq =

     []

</pre><p>The total number of function evaluations was:</p><pre class="codeinput">output.funcCount
</pre><pre class="codeoutput">
ans =

    18

</pre><h2>Constrained optimization example: equality constraints<a name="40"></a></h2>
      <p>Consider the above problem with an additional equality constraint:</p><pre>                                    2        2
 minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)</pre><pre> subject to                  x(1)^2 + x(2)  = 1
                            -x(1).x(2)     &lt;= 10</pre><p>Like in previous examples, the objective function is defined in the M-file objfun.m:</p><pre class="codeinput">type <span class="string">objfun</span>
</pre><pre class="codeoutput">
function f = objfun(x)
% Objective function

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.5.4.2 $  $Date: 2004/04/06 01:10:28 $

f = exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1);

</pre><p>The M-file confuneq.m contains the equality and inequality constraints:</p><pre class="codeinput">type <span class="string">confuneq</span>
</pre><pre class="codeoutput">
function [c, ceq] = confuneq(x)
% Nonlinear inequality constraints:

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.7.4.1 $  $Date: 2004/02/07 19:12:53 $

c = -x(1)*x(2) - 10;
% Nonlinear equality constraint:
ceq = x(1)^2 + x(2) - 1;

</pre><p>Again, make a guess at the solution:</p><pre class="codeinput">x0 = [-1 1];
</pre><p>Set optimization options: turn off the large-scale algorithms (the default):</p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
</pre><p>Call the optimization algorithm:</p><pre class="codeinput">[x,fval,exitflag,output] = fmincon(@objfun,x0,[],[],[],[],[],[],@confuneq,options);
</pre><pre class="codeoutput">Optimization terminated: first-order optimality measure less than options.TolFun
 and maximum constraint violation is less than options.TolCon.
No active inequalities
</pre><p>The solution to this problem has been found at:</p><pre class="codeinput">x
</pre><pre class="codeoutput">
x =

   -0.7529    0.4332

</pre><p>The function value at the solution is:</p><pre class="codeinput">fval
</pre><pre class="codeoutput">
fval =

    1.5093

</pre><p>The constraint values at the solution are:</p><pre class="codeinput">[c, ceq] = confuneq(x)
</pre><pre class="codeoutput">
c =

   -9.6739


ceq =

  6.3038e-009

</pre><p>The total number of function evaluations was:</p><pre class="codeinput">output.funcCount
</pre><pre class="codeoutput">
ans =

    27

</pre><h2>Changing the default termination tolerances<a name="50"></a></h2>
      <p>Consider the original unconstrained problem we solved first:</p><pre>                                    2        2
 minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)</pre><p>This time we will solve it more accurately by overriding the default termination criteria (options.TolX and options.TolFun).</p>
      <p>Create an anonymous function of the objective to be minimized:</p><pre class="codeinput">fun = @(x) exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1)
</pre><pre class="codeoutput">
fun = 

    @(x) exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1)

</pre><p>Again, make a guess at the solution:</p><pre class="codeinput">x0 = [-1; 1];
</pre><p>Set optimization options: turn off the large-scale algorithms (the default):</p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
</pre><p>Override the default termination criteria:</p><pre class="codeinput"><span class="comment">% Termination tolerance on X and f.</span>
options = optimset(options,<span class="string">'TolX'</span>,1e-3,<span class="string">'TolFun'</span>,1e-3);
</pre><p>Call the optimization algorithm:</p><pre class="codeinput">[x, fval, exitflag, output] = fminunc(fun, x0, options);
</pre><pre class="codeoutput">Optimization terminated: relative infinity-norm of gradient less than options.TolFun.
</pre><p>The optimizer has found a solution at:</p><pre class="codeinput">x
</pre><pre class="codeoutput">
x =

    0.4998
   -0.9997

</pre><p>The function value at the solution is:</p><pre class="codeinput">fval
</pre><pre class="codeoutput">
fval =

  2.0368e-007

</pre><p>The total number of function evaluations was:</p><pre class="codeinput">output.funcCount
</pre><pre class="codeoutput">
ans =

    60

</pre><p>Set optimization options: turn off the large-scale algorithms (the default):</p><pre class="codeinput">options = optimset(<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
</pre><p>If we want a tabular display of each iteration we can set options.Display = 'iter' as follows:</p><pre class="codeinput">options = optimset(options, <span class="string">'Display'</span>,<span class="string">'iter'</span>);

[x, fval, exitflag, output] = fminunc(fun, x0, options);
</pre><pre class="codeoutput">                                                         Gradient's 
 Iteration  Func-count       f(x)        Step-size      infinity-norm
     0           3           1.8394                         0.736
     1           9          1.72428       0.368157          0.257  
     2          27        0.0845289        22.5704          0.923  
     3          51         0.072564      0.0012394           1.05  
     4          54       0.00450951              1           0.29  
     5          57     1.15036e-005              1          0.014  
     6          60     2.03677e-007              1        0.00107  
     7          63     3.47346e-012              1      9.34e-006  
     8          66     1.09827e-015              1      7.37e-008  
Optimization terminated: relative infinity-norm of gradient less than options.TolFun.
</pre><p>At each major iteration the table displayed consists of: (i) number of function evaluations, (ii) function value, (iii) step
         length used in the line search, (iv) gradient in the direction of search.
      </p>
      <p class="footer">Copyright 1990-2004 The MathWorks, Inc.<br>
         Published with MATLAB&reg; 7.0<br></p>
      <!--
##### SOURCE BEGIN #####
%% Tutorial for the Optimization Toolbox.
%
% This is a demonstration for the medium-scale algorithms in the Optimization
% Toolbox. It closely follows the Tutorial section of the users' guide.
%   
% All the principles outlined in this demonstration apply to the other 
% nonlinear solvers: FGOALATTAIN, FMINIMAX, LSQNONLIN, FSOLVE.
%
% The routines differ from the Tutorial Section examples in the User's
% Guide only in that some objectives are anonymous functions instead of
% M-file functions. 
%

%   Copyright 1990-2004 The MathWorks, Inc.
%   $Revision: 1.18.4.3 $  $Date: 2004/04/06 01:10:34 $

%% Unconstrained optimization example
% Consider initially the problem of finding a minimum of the function:
%
%                                2        2
%       f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)
%
% Define the objective to be minimized as an anonymous function:

fun = @(x) exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1)

%%
% Take a guess at the solution:

x0 = [-1; 1];

%%
% Set optimization options: turn off the large-scale algorithms (the default):

options = optimset('LargeScale','off');

%%
% Call the unconstrained minimization function:

[x, fval, exitflag, output] = fminunc(fun, x0, options);

%%
% The optimizer has found a solution at:
x

%%
% The function value at the solution is:
fval

%%
% The total number of function evaluations was: 

output.funcCount

%% Constrained optimization example: inequalities
%
% Consider the above problem with two additional constraints:
%
%                                      2        2
%   minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)
%
%   subject to  1.5 + x(1).x(2) - x(1) - x(2) <= 0
%                   - x(1).x(2)               <= 10

%%
% The objective function this time is contained in an M-file, objfun.m:

type objfun

%%
% The constraints are also defined in an M-file, confun.m:

type confun

%%
% Take a guess at the solution:

x0 = [-1 1];

%%
% Set optimization options: turn off the large-scale algorithms (the 
% default) and turn on the display of results at each iteration:

options = optimset('LargeScale','off','Display','iter');

%%
% Call the optimization algorithm. We have no linear equalities or 
% inequalities or bounds, so we pass [] for those arguments:

[x,fval,exitflag,output] = fmincon(@objfun,x0,[],[],[],[],[],[],@confun,options);

%%
% A solution to this problem has been found at:

x 

%%
% The function value at the solution is: 

fval 

%%
% Both inequality constraints are satisfied (and active) at the solution:

[c, ceq] = confun(x)

%%
% The total number of function evaluations was: 

output.funcCount

%% Constrained optimization example: inequalities and bounds
%
% Consider the previous problem with additional bound constraints:
%
%                                      2        2
%   minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)
%
%   subject to  1.5 + x(1).x(2) - x(1) - x(2) <= 0
%                   - x(1).x(2)               <= 10
%
%          and                    x(1)        >= 0
%                                        x(2) >= 0

%%
% As in the previous example, the objective and constraint functions are
% defined in M-files. The file objfun.m contains the objective:

type objfun

%%
% The file confun.m contains the constraints:

type confun

%%
% Set the bounds on the variables:

lb = zeros(1,2); % Lower bounds x >= 0
ub = [];         % No upper bounds 

%%
% Again, make a guess at the solution:
  
x0 = [-1 1];

%%
% Set optimization options: turn off the large-scale algorithms (the 
% default). This time we do not turn on the Display option.

options = optimset('LargeScale','off');

%%
% Run the optimization algorithm:

[x,fval,exitflag,output] = fmincon(@objfun,x0,[],[],[],[],lb,ub,@confun,options);

%%
% The solution to this problem has been found at:

x 

%%
% The function value at the solution is: 

fval

%%
% The constraint values at the solution are:

[c, ceq] = confun(x)

%%
% The total number of function evaluations was: 

output.funcCount

%% Constrained optimization example: user-supplied gradients
%
% Optimization problems can be solved more efficiently and accurately if
% gradients are supplied by the user. This demo shows how this may be
% performed. We again solve the inequality-constrained problem
%
%                                      2        2
%   minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)
%
%   subject to  1.5 + x(1).x(2) - x(1) - x(2) <= 0
%                   - x(1).x(2)               <= 10

%%
% The objective function and its gradient are defined in the M-file 
% objfungrad.m:

type objfungrad

%%
% The constraints and their partial derivatives are contained in
% the M-file confungrad:

type confungrad 

%%
% Make a guess at the solution:

x0 = [-1; 1];

%%
% Set optimization options: since we are supplying the gradients, we have 
% the choice to use either the medium- or the large-scale algorithm; we will 
% continue to use the same algorithm for comparison purposes. 

options = optimset('LargeScale','off');

%%
% We also set options to use the gradient information in the objective
% and constraint functions. Note: these options MUST be turned on or
% the gradient information will be ignored.

options = optimset(options,'GradObj','on','GradConstr','on');

%%
% Call the optimization algorithm:

[x,fval,exitflag,output] = fmincon(@objfungrad,x0,[],[],[],[],[],[], ...
                                   @confungrad,options);

%%
% As before, the solution to this problem has been found at:

x 

%%
% The function value at the solution is: 

fval

%%
% Both inequality constraints are active at the solution:

[c, ceq] = confungrad(x)

%%
% The total number of function evaluations was: 

output.funcCount

%% Constrained optimization example: equality constraints
%
% Consider the above problem with an additional equality constraint:
%
%                                      2        2
%   minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)
%
%   subject to                  x(1)^2 + x(2)  = 1
%                              -x(1).x(2)     <= 10

%%
% Like in previous examples, the objective function is defined in the
% M-file objfun.m:

type objfun


%%
% The M-file confuneq.m contains the equality and inequality constraints:

type confuneq

%%
% Again, make a guess at the solution:

x0 = [-1 1];

%%
% Set optimization options: turn off the large-scale algorithms (the 
% default):

options = optimset('LargeScale','off');

%%
% Call the optimization algorithm:

[x,fval,exitflag,output] = fmincon(@objfun,x0,[],[],[],[],[],[],@confuneq,options);

%%
% The solution to this problem has been found at:

x 

%%
% The function value at the solution is: 

fval

%%
% The constraint values at the solution are:

[c, ceq] = confuneq(x)

%%
% The total number of function evaluations was: 

output.funcCount

%% Changing the default termination tolerances
%
% Consider the original unconstrained problem we solved first:
%
%                                      2        2
%   minimize  f(x) = exp(x(1)) . (4x(1)  + 2x(2)  + 4x(1).x(2) + 2x(2) + 1)
%
% This time we will solve it more accurately by overriding the 
% default termination criteria (options.TolX and options.TolFun). 

%%
% Create an anonymous function of the objective to be minimized:

fun = @(x) exp(x(1)) * (4*x(1)^2 + 2*x(2)^2 + 4*x(1)*x(2) + 2*x(2) + 1)

%%
% Again, make a guess at the solution:

x0 = [-1; 1];

%%
% Set optimization options: turn off the large-scale algorithms (the 
% default):

options = optimset('LargeScale','off');

%%
% Override the default termination criteria:

% Termination tolerance on X and f.
options = optimset(options,'TolX',1e-3,'TolFun',1e-3);      

%%
% Call the optimization algorithm:

[x, fval, exitflag, output] = fminunc(fun, x0, options);

%%
% The optimizer has found a solution at:

x

%%
% The function value at the solution is:

fval

%%
% The total number of function evaluations was: 

output.funcCount

%%
% Set optimization options: turn off the large-scale algorithms (the 
% default):

options = optimset('LargeScale','off');

%%
% If we want a tabular display of each iteration we can set
% options.Display = 'iter' as follows:

options = optimset(options, 'Display','iter');  

[x, fval, exitflag, output] = fminunc(fun, x0, options);

%%
% At each major iteration the table displayed consists of: (i) number of 
% function evaluations, (ii) function value, (iii) step length used in the 
% line search, (iv) gradient in the direction of search.
%



##### SOURCE END #####
-->
   </body>
</html>