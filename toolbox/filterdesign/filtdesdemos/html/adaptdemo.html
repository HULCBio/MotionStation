<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Getting Started with Adaptive Filters (ADAPFILT) Objects</title>
      <meta name="generator" content="MATLAB 7.0">
      <meta name="date" content="2004-04-19">
      <meta name="m-file" content="adaptdemo"><style>
body {
  background-color: white;
  margin:10px;
}
h1 {
  color: #990000; 
  font-size: x-large;
}
h2 {
  color: #990000;
  font-size: medium;
}
p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

pre.codeinput {
  margin-left: 30px;
}

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.showbuttons {
  margin-left: 30px;
  border: solid black 2px;
  padding: 4px;
  background: #EBEFF3;
}

pre.codeoutput {
  color: gray;
  font-style: italic;
}
pre.error {
  color: red;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows.  On Gecko-based browsers, the shrink-to-fit doesn't work. */ 
p,h1,h2,div {
  /* for MATLAB's browser */
  width: 600px;
  /* for Mozilla, but the "width" tag overrides it anyway */
  max-width: 600px;
  /* for IE */
  width:expression(document.body.clientWidth > 620 ? "600px": "auto" );
}

    </style></head>
   <body>
      <h1>Getting Started with Adaptive Filters (ADAPFILT) Objects</h1>
      <introduction>
         <p>This demonstration illustrates how to use the adaptive filter algorithms provided in the Filter Design Toolbox.</p>
         <p>We will use a very simple signal enhancement application as an illustration. While we demonstrate with two algorithms only,
            there are about 30 different adaptive filtering algorithms included with the Filter Design Toolbox.
         </p>
      </introduction>
      <h2>Contents</h2>
      <div>
         <ul>
            <li><a href="#1">Getting Help</a></li>
            <li><a href="#2">Desired Signal</a></li>
            <li><a href="#3">Noise Signal</a></li>
            <li><a href="#4">Noisy Signal</a></li>
            <li><a href="#5">Reference Signal</a></li>
            <li><a href="#6">Constructing Adaptive Filters</a></li>
            <li><a href="#7">Choosing the Step Size</a></li>
            <li><a href="#8">Setting the Step Size</a></li>
            <li><a href="#10">Filtering with Adaptive Filters</a></li>
            <li><a href="#11">Optimal Solution</a></li>
            <li><a href="#12">Plot of Results</a></li>
            <li><a href="#14">Final Coefficients</a></li>
            <li><a href="#15">Resetting the Filter Before Filtering</a></li>
            <li><a href="#18">Learning Curves</a></li>
            <li><a href="#19">Computing the Learning Curves</a></li>
            <li><a href="#20">Theoretical Learning Curves</a></li>
         </ul>
      </div>
      <h2>Getting Help<a name="1"></a></h2>
      <p>To see a list of all algorithms available along with all the methods that are applicable to adaptive filters type "helpwin
         adaptfilt" or "help adaptfilt".
      </p>
      <p>To get help on a specific algorithm you can type "helpwin adaptfilt/lms" for example.</p>
      <p>We now provide a simple example.</p>
      <h2>Desired Signal<a name="2"></a></h2>
      <p>We want to use an adaptive filter to extract a desired signal from a noise corrupted signal by cancelling the noise. The desired
         signal is a sinusoid
      </p><pre class="codeinput">n = (1:1000)';
s = sin(0.075*pi*n);
</pre><h2>Noise Signal<a name="3"></a></h2>
      <p>We assume that the noise signal v1 is autoregresive.</p><pre class="codeinput">v = 0.8*randn(1000,1);
ar = [1, 1/2];
v1 = filter(1,ar,v);
</pre><h2>Noisy Signal<a name="4"></a></h2>
      <p>The noise corrupted sinusoid is simply s + v1</p><pre class="codeinput">x = s + v1;
</pre><h2>Reference Signal<a name="5"></a></h2>
      <p>We assume we have a moving average signal v2 that is correlated with v1. This will be used as the reference signal.</p><pre class="codeinput">ma = [1, -0.8, 0.4 , -0.2];
v2 = filter(ma,1,v);
</pre><h2>Constructing Adaptive Filters<a name="6"></a></h2>
      <p>We will use two adaptive filters each with 7 taps (wieghts), an LMS filter, and a normalized LMS (NLMS) filter.</p>
      <p>Type "helpwin adaptfilt/lms" and "helpwin adaptfilt/nlms" for further information on constructing LMS and NLMS adaptive filters.</p><pre class="codeinput">L = 7;
hlms = adaptfilt.lms(7);
hnlms = adaptfilt.nlms(7);
</pre><h2>Choosing the Step Size<a name="7"></a></h2>
      <p>All the LMS-like algorithm have a so-called step size which determines the amount of correction that is done as the filter
         adapts from one iteration to the next. Choosing the approprite step size is not always easy, a step size that is too small,
         will hamper convergence speed, while one that is too large may cause the filter to diverge. The Filter Design Toolbox includes
         algorithms to determine the maximum step size allowed for these algorithms while ensuring convergence.
      </p>
      <p>Type "helpwin adaptfilt/maxstep" for more information.</p><pre class="codeinput">[mumaxlms,mumaxmselms]   = maxstep(hlms,x)
[mumaxnlms,mumaxmsenlms] = maxstep(hnlms); <span class="comment">% Always equal to two</span>
</pre><pre class="codeoutput">Warning: Step size is not in the range 0 &lt; mu &lt; mumaxmse/2: 
Erratic behavior might result.

mumaxlms =

    0.2270


mumaxmselms =

    0.1356

</pre><h2>Setting the Step Size<a name="8"></a></h2>
      <p>The first output of maxstep is the value needed for the coefficients mean to converge, while the second one is the value needed
         for the mean squared coefficients to converge. However, choosing a large step size also results in large variations from the
         convergence values, so we choose smaller step sizes
      </p><pre class="codeinput">hlms.step = mumaxmselms/30; <span class="comment">% This can also be set graphically: inspect(hlms)</span>
hnlms.step = mumaxmsenlms/20; <span class="comment">% This can also be set graphically: inspect(hnlms)</span>
</pre><p>If we already know the step size we want to use, we can also set its value when we first create the filter: hlms = adaptfilt.lms(N,step);</p>
      <h2>Filtering with Adaptive Filters<a name="10"></a></h2>
      <p>Now that we have setup the parameters of the adaptive filters, we can filter the noisy signal. The reference signal, v2 will
         be the input to the adaptive filters, while x is the desired signal in this setup. The output of the filters, y will try to
         emulate x as best possible. However, since the input to the adaptive filter, v2, is only correlated with the noise component
         of x, v1, it can only really emulate this. The error signal, that is the desired x, minus the actual output y will thus constitute
         an estimate of the part of x that is not correlated with v2 namely s which is the signal we want to extract from x.
      </p>
      <p>Type "helpwin adaptfilt/filter" for a complete description of filtering with adaptive filters.</p><pre class="codeinput">[ylms,elms] = filter(hlms,v2,x);
[ynlms,enlms] = filter(hnlms,v2,x);
</pre><h2>Optimal Solution<a name="11"></a></h2>
      <p>For comparison, we compute the optimal FIR Wiener filter</p><pre class="codeinput">bw = firwiener(L-1,v2,x); <span class="comment">% Optimal FIR Wiener filter</span>
yw = filter(bw,1,v2);   <span class="comment">% Estimate of x using Wiener filter</span>
ew = x - yw;            <span class="comment">% Estimate of actual sinusoid</span>
</pre><h2>Plot of Results<a name="12"></a></h2>
      <p>We now show the estimated sinusoid for each case</p><pre class="codeinput">plot(n(900:end),[ew(900:end), elms(900:end),enlms(900:end)]);
legend(<span class="string">'Wiener filter denoised sinusoid'</span>,<span class="keyword">...</span>
    <span class="string">'LMS denoised sinusoid'</span>, <span class="string">'NLMS denoised sinusoid'</span>);
</pre><img vspace="5" hspace="5" src="adaptdemo_01.png"> <p>As a reference, we show the noisy signal dotted</p><pre class="codeinput">hold <span class="string">on</span>
plot(n(900:end),x(900:end),<span class="string">'k:'</span>)
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="adaptdemo_02.png"> <h2>Final Coefficients<a name="14"></a></h2>
      <p>We can compare the Wiener filter coefficients with those of the adaptive filters. The adaptive filters will try to converge
         to the Wiener coefficients
      </p><pre class="codeinput">[bw.' hlms.Coefficients.' hnlms.Coefficients.']
</pre><pre class="codeoutput">
ans =

    1.0221    0.8751    1.0411
    0.3345    0.1201    0.3601
    0.1217   -0.0118    0.1077
    0.0483   -0.0183    0.0081
    0.1179    0.0558    0.0420
    0.0637   -0.0049   -0.0290
    0.0216   -0.0235   -0.0222

</pre><h2>Resetting the Filter Before Filtering<a name="15"></a></h2>
      <p>The adaptive fiters have a "ResetBeforeFiltering" flag that can be used to reproduce experiments exactly. By default, the
         flag is "on", which means that the states and the coefficients of the filter are reset before filtering. For instance, the
         following two succesive calls produce the same output in both cases.
      </p><pre class="codeinput">[ylms,elms] = filter(hlms,v2,x);
[ylms2,elms2] = filter(hlms,v2,x);
</pre><p>To keep the history of the filter when filtering a new chunk of data, we must set the flag to "off". In this case, the final
         states and coefficients of a previous run are used as initial conditions for the next set of data.
      </p><pre class="codeinput">[ylms,elms] = filter(hlms,v2,x);
hlms.ResetBeforeFiltering = <span class="string">'off'</span>;
[ylms2,elms2] = filter(hlms,v2,x); <span class="comment">% No longer the same</span>
</pre><p>Setting the flag to "off" can be useful when filtering large amounts of data which can be partioned into smaller chunks and
         then fed into the filter using a for-loop.
      </p>
      <h2>Learning Curves<a name="18"></a></h2>
      <p>To analyze the convergence of the adaptive filters, we look at the so-called learning curves. These can easily be generated
         in the Filter Design Toolbox, but we need more experiments to obtain significant results. We will use 25 sample realizations
         of the noisy sinusoids.
      </p><pre class="codeinput">n = (1:5000)';
s = sin(0.075*pi*n);
nr = 25;
v = 0.8*randn(5000,nr);
v1 = filter(1,ar,v);
x = repmat(s,1,nr) + v1;
v2 = filter(ma,1,v);
</pre><h2>Computing the Learning Curves<a name="19"></a></h2>
      <p>We now compute the mean-square error. To speed things up, we only compute the error every 10 samples. First we need to reset
         the adaptive filters to avoid using the coefficients it has already computed and the states it has stored.
      </p><pre class="codeinput">reset(hlms);
reset(hnlms);
M = 10; <span class="comment">% Decimation factor</span>
mselms = msesim(hlms,v2,x,M);
msenlms = msesim(hnlms,v2,x,M);
plot(1:M:n(end),[mselms,msenlms])
legend(<span class="string">'LMS learning curve'</span>,<span class="string">'NLMS learning curve'</span>)
</pre><img vspace="5" hspace="5" src="adaptdemo_03.png"> <h2>Theoretical Learning Curves<a name="20"></a></h2>
      <p>For the LMS and NLMS algorithms, the theoretical learning curves can also be computed, along with the minimum mean-square
         error (MMSE) the excess mean-square error (EMSE) and the mean value of the coefficients.
      </p>
      <p>This may take a while.</p><pre class="codeinput">reset(hlms);
[mmselms,emselms,meanwlms,pmselms] = msepred(hlms,v2,x,M);
plot(1:M:n(end),[mmselms*ones(500,1),emselms*ones(500,1),<span class="keyword">...</span>
        pmselms,mselms])
legend(<span class="string">'MMSE'</span>,<span class="string">'EMSE'</span>,<span class="string">'predicted LMS learning curve'</span>,<span class="keyword">...</span>
    <span class="string">'LMS learning curve'</span>)
</pre><img vspace="5" hspace="5" src="adaptdemo_04.png"> <p class="footer">Copyright 1999-2004 The MathWorks, Inc.<br>
         Published with MATLAB&reg; 7.0<br></p>
      <!--
##### SOURCE BEGIN #####
%% Getting Started with Adaptive Filters (ADAPFILT) Objects
% This demonstration illustrates how to use the adaptive filter algorithms
% provided in the Filter Design Toolbox.
%
% We will use a very simple signal enhancement application as an
% illustration. While we demonstrate with two algorithms only, there are
% about 30 different adaptive filtering algorithms included with the Filter
% Design Toolbox.
% 
% Copyright 1999-2004 The MathWorks, Inc. 
% $Revision: 1.1.10.1 $ $Date: 2004/04/08 16:17:54 $

%% Getting Help
% To see a list of all algorithms available along with all the methods that
% are applicable to adaptive filters type "helpwin adaptfilt" or "help
% adaptfilt".
%
% To get help on a specific algorithm you can type "helpwin adaptfilt/lms"
% for example.
%
% We now provide a simple example.


%% Desired Signal
% We want to use an adaptive filter to extract a desired signal from a
% noise corrupted signal by cancelling the noise. The desired signal is a
% sinusoid

n = (1:1000)';
s = sin(0.075*pi*n);

%% Noise Signal
% We assume that the noise signal v1 is autoregresive.

v = 0.8*randn(1000,1);
ar = [1, 1/2];
v1 = filter(1,ar,v);

%% Noisy Signal
% The noise corrupted sinusoid is simply s + v1

x = s + v1;

%% Reference Signal
% We assume we have a moving average signal v2 that is correlated with
% v1. This will be used as the reference signal.

ma = [1, -0.8, 0.4 , -0.2];
v2 = filter(ma,1,v);

%% Constructing Adaptive Filters
% We will use two adaptive filters each with 7 taps (wieghts), an LMS filter, and a normalized
% LMS (NLMS) filter.
%
% Type "helpwin adaptfilt/lms" and "helpwin adaptfilt/nlms" for further
% information on constructing LMS and NLMS adaptive filters.

L = 7;
hlms = adaptfilt.lms(7);
hnlms = adaptfilt.nlms(7);

%% Choosing the Step Size
% All the LMS-like algorithm have a so-called step size which determines
% the amount of correction that is done as the filter adapts from one
% iteration to the next. Choosing the approprite step size is not always
% easy, a step size that is too small, will hamper convergence speed, while
% one that is too large may cause the filter to diverge. The Filter Design
% Toolbox includes algorithms to determine the maximum step size allowed
% for these algorithms while ensuring convergence.
%
% Type "helpwin adaptfilt/maxstep" for more information.


[mumaxlms,mumaxmselms]   = maxstep(hlms,x)
[mumaxnlms,mumaxmsenlms] = maxstep(hnlms); % Always equal to two

%% Setting the Step Size
% The first output of maxstep is the value needed for the coefficients mean
% to converge, while the second one is the value needed for the mean
% squared coefficients to converge. However, choosing a large step size
% also results in large variations from the convergence values, so we
% choose smaller step sizes

hlms.step = mumaxmselms/30; % This can also be set graphically: inspect(hlms)
hnlms.step = mumaxmsenlms/20; % This can also be set graphically: inspect(hnlms)

%%
% If we already know the step size we want to use, we can also set its
% value when we first create the filter: hlms = adaptfilt.lms(N,step);

%% Filtering with Adaptive Filters
% Now that we have setup the parameters of the adaptive filters, we can
% filter the noisy signal. The reference signal, v2 will be the input to
% the adaptive filters, while x is the desired signal in this setup. The
% output of the filters, y will try to emulate x as best possible. However,
% since the input to the adaptive filter, v2, is only correlated with the
% noise component of x, v1, it can only really emulate this. The error
% signal, that is the desired x, minus the actual output y will thus
% constitute an estimate of the part of x that is not correlated with v2
% namely s which is the signal we want to extract from x.
%
% Type "helpwin adaptfilt/filter" for a complete description of filtering
% with adaptive filters.

[ylms,elms] = filter(hlms,v2,x);
[ynlms,enlms] = filter(hnlms,v2,x);

%% Optimal Solution
% For comparison, we compute the optimal FIR Wiener filter

bw = firwiener(L-1,v2,x); % Optimal FIR Wiener filter
yw = filter(bw,1,v2);   % Estimate of x using Wiener filter
ew = x - yw;            % Estimate of actual sinusoid

%% Plot of Results
% We now show the estimated sinusoid for each case

plot(n(900:end),[ew(900:end), elms(900:end),enlms(900:end)]);
legend('Wiener filter denoised sinusoid',...
    'LMS denoised sinusoid', 'NLMS denoised sinusoid');

%% 
% As a reference, we show the noisy signal dotted

hold on
plot(n(900:end),x(900:end),'k:')
hold off

%% Final Coefficients
% We can compare the Wiener filter coefficients with those of the adaptive
% filters. The adaptive filters will try to converge to the Wiener
% coefficients

[bw.' hlms.Coefficients.' hnlms.Coefficients.']

%% Resetting the Filter Before Filtering
% The adaptive fiters have a "ResetBeforeFiltering" flag that can be used
% to reproduce experiments exactly. By default, the flag is "on", which
% means that the states and the coefficients of the filter are reset before
% filtering. For instance, the following two succesive calls produce the
% same output in both cases.

[ylms,elms] = filter(hlms,v2,x);
[ylms2,elms2] = filter(hlms,v2,x);

%%
% To keep the history of the filter when filtering a new chunk of data, we
% must set the flag to "off". In this case, the final states and
% coefficients of a previous run are used as initial conditions for the
% next set of data.

[ylms,elms] = filter(hlms,v2,x);
hlms.ResetBeforeFiltering = 'off';
[ylms2,elms2] = filter(hlms,v2,x); % No longer the same

%%
% Setting the flag to "off" can be useful when filtering large amounts of
% data which can be partioned into smaller chunks and then fed into the
% filter using a for-loop.

%% Learning Curves
% To analyze the convergence of the adaptive filters, we look at the
% so-called learning curves. These can easily be generated in the Filter
% Design Toolbox, but we need more experiments to obtain significant
% results. We will use 25 sample realizations of the noisy sinusoids.

n = (1:5000)';
s = sin(0.075*pi*n);
nr = 25;
v = 0.8*randn(5000,nr);
v1 = filter(1,ar,v);
x = repmat(s,1,nr) + v1;
v2 = filter(ma,1,v);


%% Computing the Learning Curves
% We now compute the mean-square error. To speed things up, we only compute
% the error every 10 samples. First we need to reset the adaptive filters
% to avoid using the coefficients it has already computed and the states it
% has stored.

reset(hlms);
reset(hnlms);
M = 10; % Decimation factor
mselms = msesim(hlms,v2,x,M);
msenlms = msesim(hnlms,v2,x,M);
plot(1:M:n(end),[mselms,msenlms])
legend('LMS learning curve','NLMS learning curve')

%% Theoretical Learning Curves
% For the LMS and NLMS algorithms, the theoretical learning curves can also
% be computed, along with the minimum mean-square error (MMSE) the excess
% mean-square error (EMSE) and the mean value of the coefficients.
%
% This may take a while.

reset(hlms);
[mmselms,emselms,meanwlms,pmselms] = msepred(hlms,v2,x,M);
plot(1:M:n(end),[mmselms*ones(500,1),emselms*ones(500,1),...
        pmselms,mselms])
legend('MMSE','EMSE','predicted LMS learning curve',...
    'LMS learning curve')



##### SOURCE END #####
-->
   </body>
</html>