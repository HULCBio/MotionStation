<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Linear prediction and autoregressive (AR) modeling</title>
      <meta name="generator" content="MATLAB 7.0">
      <meta name="date" content="2004-04-19">
      <meta name="m-file" content="lpc-ardemo"><style>
body {
  background-color: white;
  margin:10px;
}
h1 {
  color: #990000; 
  font-size: x-large;
}
h2 {
  color: #990000;
  font-size: medium;
}
p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

pre.codeinput {
  margin-left: 30px;
}

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.showbuttons {
  margin-left: 30px;
  border: solid black 2px;
  padding: 4px;
  background: #EBEFF3;
}

pre.codeoutput {
  color: gray;
  font-style: italic;
}
pre.error {
  color: red;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows.  On Gecko-based browsers, the shrink-to-fit doesn't work. */ 
p,h1,h2,div {
  /* for MATLAB's browser */
  width: 600px;
  /* for Mozilla, but the "width" tag overrides it anyway */
  max-width: 600px;
  /* for IE */
  width:expression(document.body.clientWidth > 620 ? "600px": "auto" );
}

    </style></head>
   <body>
      <h1>Linear prediction and autoregressive (AR) modeling</h1>
      <introduction>
         <p>This demo is intended to show the relationship between autoregressive modeling and linear prediction. Linear prediction and
            autoregressive modeling are two different problems that can yield the same numerical results. In both cases, the ultimate
            goal is to determine the parameters of a linear filter. However, the filter used in each problem is different.
         </p>
         <p>Copyright 1988-2004 The Mathworks, Inc.</p>
      </introduction>
      <h2>Contents</h2>
      <div>
         <ul>
            <li><a href="#1">Introduction</a></li>
            <li><a href="#2">Generate an AR Signal using an All-Pole Filter with White Noise as Input</a></li>
            <li><a href="#5">Find AR Model from Signal using the Yule-Walker Method</a></li>
            <li><a href="#6">Compare AR Model with AR Signal</a></li>
            <li><a href="#8">Use LPC to Perform Linear Prediction</a></li>
            <li><a href="#10">Compare Actual and Predicted Signals</a></li>
            <li><a href="#11">Compare Prediction Errors</a></li>
         </ul>
      </div>
      <h2>Introduction<a name="1"></a></h2>
      <p>In the case of linear prediction, the intention is to determine an FIR filter that can optimally predict future samples of
         an autoregressive process based on a linear combination of past samples. The difference between the actual autoregressive
         signal and the predicted signal is called the prediction error. Ideally, this error is white noise.
      </p>
      <p>For the case of autoregressive modeling, the intention is to determine an all-pole IIR filter, that when excited with white
         noise produces a signal with the same statistics as the autoregresive process that we are trying to model.
      </p>
      <h2>Generate an AR Signal using an All-Pole Filter with White Noise as Input<a name="2"></a></h2>
      <p>Here we use the LPC function and an FIR filter simply to come up with parameters we will use to create the autoregressive
         signal we will work with. The use of FIR1 and LPC are not critical here. For example, we could replace d with something as
         simple as [1 1/2 1/3 1/4 1/5 1/6 1/7 1/8] and p0 with something like 1e-6. But the shape of this filter is nicer so we use
         it instead.
      </p><pre class="codeinput">b = fir1(1024, .5);
[d,p0] = lpc(b,7);
</pre><p>To generate the autoregressive signal, we will excite an all-pole filter with white gaussian noise of variance p0. Notice
         that to get variance p0, we must use SQRT(p0) as the 'gain' term in the noise generator.
      </p><pre class="codeinput">randn(<span class="string">'state'</span>,pi); <span class="comment">% Allow reproduction of exact experiment</span>
u = sqrt(p0)*randn(8192,1); <span class="comment">% White gaussian noise with variance p0</span>
</pre><p>We now use the white gaussian noise signal and the all-pole filter to generate an AR signal.</p><pre class="codeinput">x = filter(1,d,u);
</pre><h2>Find AR Model from Signal using the Yule-Walker Method<a name="5"></a></h2>
      <p>Solving the Yule-Walker equations, we can determine the parameters for an all-pole filter that when excited with white noise
         will produce an AR signal whose statistics match those of the given signal, x. Once again, this is called autoregressive modeling.
         In order to solve the Yule-Walker equations, it is necessary to estimate the autocorrelation function of x. The Levinson algorithm
         is used then to solve the Yule-Walker equations in an efficient manner. The function ARYULE does all this for us.
      </p><pre class="codeinput">[d1,p1] = aryule(x,7);
</pre><h2>Compare AR Model with AR Signal<a name="6"></a></h2>
      <p>We now would like to compute the frequency response of the all-pole filter we have just used to model the AR signal x. It
         is well-known that the power spectral density of the output of this filter, when the filter is excited with white gaussian
         noise is given by the magnitude-squared of its frequency response multiplied by the variance of the white-noise input. One
         way to compute this output power spectral density is by using FREQZ as follows:
      </p><pre class="codeinput">[H1,w1]=freqz(sqrt(p1),d1);
</pre><p>In order to get an idea of how well we have modeled the autoregressive signal x, we overlay the power spectral density of
         the output of the model, computed using FREQZ, with the power spectral density estimate of x, computed using the PERIODOGRAM
         spectrum object. Notice that the periodogram is scaled by 2*pi and is one-sided. We need to adjust for this in order to compare.
      </p><pre class="codeinput">s = spectrum.periodogram;
Hpsd = psd(s,x);
plot(Hpsd); hold <span class="string">on</span>;
hp = plot(w1/pi,20*log10(2*abs(H1)/(2*pi)),<span class="string">'r'</span>); <span class="comment">% Scale to make one-sided PSD</span>
set(hp,<span class="string">'LineWidth'</span>,2);
xlabel(<span class="string">'Normalized frequency (\times \pi rad/sample)'</span>)
ylabel(<span class="string">'One-sided PSD (dB/rad/sample)'</span>)
legend(<span class="string">'PSD estimate of x'</span>,<span class="string">'PSD of model output'</span>)
</pre><img vspace="5" hspace="5" src="lpc-ardemo_01.png"> <h2>Use LPC to Perform Linear Prediction<a name="8"></a></h2>
      <p>We now turn to the linear prediction problem. Here we try to determine an FIR prediction filter. We use LPC to do so, but
         the result from LPC requires a little interpretation. LPC returns the coefficients of the entire whitening filter A(z), this
         filter takes as input the autoregressive signal x and returns as output the prediction error. However, A(z) has the prediction
         filter embedded in it, in the form B(z) = 1- A(z), where B(z) is the prediction filter. Note that the coefficients and error
         variance computed with LPC are essentially the same as those computed with ARYULE, but their interpretation is different.
      </p><pre class="codeinput">[d2,p2] = lpc(x,7);
[d1.', d2.']
</pre><pre class="codeoutput">
ans =

    1.0000    1.0000
   -3.5020   -3.5020
    6.8764    6.8764
   -9.1668   -9.1668
    8.7773    8.7773
   -6.0146   -6.0146
    2.7617    2.7617
   -0.6811   -0.6811

</pre><p>We now extract B(z) from A(z) as described above to use the FIR linear predictor filter to obtain an estimate of future values
         of the autoregressive signal based on linear combinations of past values.
      </p><pre class="codeinput">xh=filter(-d2(2:end),1,x);
</pre><h2>Compare Actual and Predicted Signals<a name="10"></a></h2>
      <p>To get a feeling for what we have done with a 7-tap FIR prediction filter, we plot (200 samples) of the original autoregressive
         signal along with the signal estimate resulting from the linear predictor keeping in mind the one-sample delay in the prediction
         filter.
      </p><pre class="codeinput">cla
stem([x(2:end),xh(1:end-1)]);
xlabel(<span class="string">'Sample time'</span>);
ylabel(<span class="string">'Signal value'</span>);
legend(<span class="string">'Original autoregressive signal'</span>,<span class="string">'Signal estimate from linear predictor'</span>)
axis([0 200 -0.08 0.1])
</pre><img vspace="5" hspace="5" src="lpc-ardemo_02.png"> <h2>Compare Prediction Errors<a name="11"></a></h2>
      <p>The prediction error power (variance) is returned as the second output from LPC. Its value is (theoretically) the same as
         the variance of the white noise driving the all-pole filter in the AR modeling problem (p1). Another way of estimating this
         variance is from the prediction error itself:
      </p><pre class="codeinput">p3 = norm(x(2:end)-xh(1:end-1),2)^2/(length(x)-1);
</pre><p>All of the following values are theoretically the same. The differences are due to the various computation and approximation
         errors herein.
      </p><pre class="codeinput">[p0,p1,p2,p3]
</pre><pre class="codeoutput">
ans =

  1.0e-005 *

    0.5127    0.5517    0.5517    0.5192

</pre><p class="footer"><br>
         Published with MATLAB&reg; 7.0<br></p>
      <!--
##### SOURCE BEGIN #####
%% Linear prediction and autoregressive (AR) modeling
% This demo is intended to show the relationship between autoregressive
% modeling and linear prediction. Linear prediction and autoregressive
% modeling are two different problems that can yield the same numerical
% results. In both cases, the ultimate goal is to determine the parameters
% of a linear filter. However, the filter used in each problem is
% different.
%
% Copyright 1988-2004 The Mathworks, Inc.
% $Revision: 1.1.4.3.4.2 $  $Date: 2004/04/14 16:02:05 $

%% Introduction
% In the case of linear prediction, the intention is to determine an FIR
% filter that can optimally predict future samples of an autoregressive
% process based on a linear combination of past samples. The difference
% between the actual autoregressive signal and the predicted signal is
% called the prediction error. Ideally, this error is white noise.
%
% For the case of autoregressive modeling, the intention is to determine an
% all-pole IIR filter, that when excited with white noise produces a signal
% with the same statistics as the autoregresive process that we are trying
% to model.

%% Generate an AR Signal using an All-Pole Filter with White Noise as Input
% Here we use the LPC function and an FIR filter simply to come up with
% parameters we will use to create the autoregressive signal we will work
% with. The use of FIR1 and LPC are not critical here. For example, we
% could replace d with something as simple as [1 1/2 1/3 1/4 1/5 1/6 1/7
% 1/8] and p0 with something like 1e-6. But the shape of this filter is
% nicer so we use it instead.
b = fir1(1024, .5);
[d,p0] = lpc(b,7);

%% 
% To generate the autoregressive signal, we will excite an all-pole filter
% with white gaussian noise of variance p0. Notice that to get variance p0,
% we must use SQRT(p0) as the 'gain' term in the noise generator.
randn('state',pi); % Allow reproduction of exact experiment
u = sqrt(p0)*randn(8192,1); % White gaussian noise with variance p0

%% 
% We now use the white gaussian noise signal and the all-pole filter to
% generate an AR signal.
x = filter(1,d,u);

%% Find AR Model from Signal using the Yule-Walker Method
% Solving the Yule-Walker equations, we can determine the parameters for an
% all-pole filter that when excited with white noise will produce an AR
% signal whose statistics match those of the given signal, x. Once again,
% this is called autoregressive modeling. In order to solve the Yule-Walker
% equations, it is necessary to estimate the autocorrelation function of x.
% The Levinson algorithm is used then to solve the Yule-Walker equations in
% an efficient manner. The function ARYULE does all this for us.
[d1,p1] = aryule(x,7);

%% Compare AR Model with AR Signal
% We now would like to compute the frequency response of the all-pole
% filter we have just used to model the AR signal x. It is well-known that
% the power spectral density of the output of this filter, when the filter
% is excited with white gaussian noise is given by the magnitude-squared of
% its frequency response multiplied by the variance of the white-noise
% input. One way to compute this output power spectral density is by using
% FREQZ as follows:
[H1,w1]=freqz(sqrt(p1),d1);

%% 
% In order to get an idea of how well we have modeled the autoregressive
% signal x, we overlay the power spectral density of the output of the
% model, computed using FREQZ, with the power spectral density estimate of
% x, computed using the PERIODOGRAM spectrum object. Notice that the
% periodogram is scaled by 2*pi and is one-sided. We need to adjust for
% this in order to compare.
s = spectrum.periodogram;
Hpsd = psd(s,x);
plot(Hpsd); hold on;
hp = plot(w1/pi,20*log10(2*abs(H1)/(2*pi)),'r'); % Scale to make one-sided PSD
set(hp,'LineWidth',2);
xlabel('Normalized frequency (\times \pi rad/sample)')
ylabel('One-sided PSD (dB/rad/sample)')
legend('PSD estimate of x','PSD of model output')

%% Use LPC to Perform Linear Prediction
% We now turn to the linear prediction problem. Here we try to determine an
% FIR prediction filter. We use LPC to do so, but the result from LPC
% requires a little interpretation. LPC returns the coefficients of the
% entire whitening filter A(z), this filter takes as input the
% autoregressive signal x and returns as output the prediction error.
% However, A(z) has the prediction filter embedded in it, in the form B(z)
% = 1- A(z), where B(z) is the prediction filter. Note that the
% coefficients and error variance computed with LPC are essentially the
% same as those computed with ARYULE, but their interpretation is different.
[d2,p2] = lpc(x,7);
[d1.', d2.']

%% 
% We now extract B(z) from A(z) as described above to use the FIR linear
% predictor filter to obtain an estimate of future values of the
% autoregressive signal based on linear combinations of past values.
xh=filter(-d2(2:end),1,x);

%% Compare Actual and Predicted Signals
% To get a feeling for what we have done with a 7-tap FIR prediction
% filter, we plot (200 samples) of the original autoregressive signal along
% with the signal estimate resulting from the linear predictor keeping in
% mind the one-sample delay in the prediction filter.
cla
stem([x(2:end),xh(1:end-1)]);
xlabel('Sample time');
ylabel('Signal value');
legend('Original autoregressive signal','Signal estimate from linear predictor')
axis([0 200 -0.08 0.1])

%% Compare Prediction Errors
% The prediction error power (variance) is returned as the second output
% from LPC. Its value is (theoretically) the same as the variance of the
% white noise driving the all-pole filter in the AR modeling problem (p1).
% Another way of estimating this variance is from the prediction error
% itself:
p3 = norm(x(2:end)-xh(1:end-1),2)^2/(length(x)-1);


%%
% All of the following values are theoretically the same. The
% differences are due to the various computation and approximation errors
% herein.
[p0,p1,p2,p3]


##### SOURCE END #####
-->
   </body>
</html>