<!--
This HTML is auto-generated from an m-file.
Your changes will be overwritten.
--><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:x-large">Normalized Perceptron Rule</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">A 2-input hard limit neuron is trained to classify 5 input vectors into two
categories.  Despite the fact that one input vector is much bigger than the
others, training with LEARNPN is quick.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Copyright 1992-2002 The MathWorks, Inc.
$Revision: 1.16 $  $Date: 2002/03/29 19:36:09 $
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Each of the five column vectors in P defines a 2-element input vectors, and a
row vector T defines the vector's target categories.  Plot these vectors with
PLOTPV.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">P = [ -0.5 -0.5 +0.3 -0.1 -40; <span style="color:blue">...</span>
      -0.5 +0.5 -0.5 +1.0 50];
T = [1 1 0 0 1];
plotpv(P,T);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop5_img02.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Note that 4 input vectors have much smaller magnitudes than the fifth vector
in the upper left of the plot.  The perceptron must properly classify the 5
input vectors in P into the two categories defined by T.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">NEWP creates aperceptron.  The first argument specifies the expected ranges of
two inputs.  The second argument determines that there is only one neuron in
the layer. LEARNPN is less sensitive to large variations in input vector size
than LEARNP (the default).
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">net = newp([-40 1;-1 50],1,<span style="color:#B20000">'hardlim'</span>,<span style="color:#B20000">'learnpn'</span>);</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Add the the neuron's initial attempt at classification to the plot.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">The initial weights are set to zero, so any input gives the same output and
the classification line does not even appear on the plot.   Fear not... we are
going to train it!
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">hold on
linehandle = plotpc(net.IW{1},net.b{1});</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop5_img04.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">ADAPT returns a new network object that performs as a better classifier, the
network output, and the error.  This loop allows the network to adapt for 3
passes, plots the classification line, and continues until the error is zero.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">E = 1;
net.adaptParam.passes = 3;
<span style="color:blue">while</span> (sse(E))
   [net,Y,E] = adapt(net,P,T);
   linehandle = plotpc(net.IW{1},net.b{1},linehandle);
   drawnow;
<span style="color:blue">end</span></pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop5_img05.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Note that training with LEARNP took only 3 epochs, while solving the same
problem with LEARNPN required 32 epochs.  Thus, LEARNPN does much better job
than LEARNP when there are large variations in input vector size.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Now SIM can be used to classify any other input vector. For example, classify
an input vector of [0.7; 1.2].
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">A plot of this new point with the original training set shows how the network
performs.  To distinguish it from the training set, color it red.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">p = [0.7; 1.2];
a = sim(net,p);
plotpv(p,a);
circle = findobj(gca,<span style="color:#B20000">'type'</span>,<span style="color:#B20000">'line'</span>);
set(circle,<span style="color:#B20000">'Color'</span>,<span style="color:#B20000">'red'</span>);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop5_img07.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Turn on "hold" so the previous plot is not erased.  Add the training set
and the classification line to the plot.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">hold on;
plotpv(P,T);
plotpc(net.IW{1},net.b{1});
hold off;</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop5_img08.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Finally, zoom into the area of interest.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">The perceptron correctly classified our new point (in red) as category "zero"
(represented by a circle) and not a "one" (represented by a plus). The
perceptron learns properly in much shorter time in spite of the outlier
(compare with the "Outlier Input Vectors" demo).
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">axis([-2 2 -2 2]);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop5_img09.gif"><originalCode xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" code="%% Normalized Perceptron Rule&#xA;% A 2-input hard limit neuron is trained to classify 5 input vectors into two&#xA;% categories.  Despite the fact that one input vector is much bigger than the&#xA;% others, training with LEARNPN is quick.&#xA;%&#xA;% Copyright 1992-2002 The MathWorks, Inc.&#xA;% $Revision: 1.16 $  $Date: 2002/03/29 19:36:09 $&#xA;&#xA;%%&#xA;% Each of the five column vectors in P defines a 2-element input vectors, and a&#xA;% row vector T defines the vector's target categories.  Plot these vectors with&#xA;% PLOTPV.&#xA;&#xA;P = [ -0.5 -0.5 +0.3 -0.1 -40; ...&#xA;      -0.5 +0.5 -0.5 +1.0 50];&#xA;T = [1 1 0 0 1];&#xA;plotpv(P,T);&#xA;&#xA;&#xA;%%&#xA;% Note that 4 input vectors have much smaller magnitudes than the fifth vector&#xA;% in the upper left of the plot.  The perceptron must properly classify the 5&#xA;% input vectors in P into the two categories defined by T.  &#xA;% &#xA;% NEWP creates aperceptron.  The first argument specifies the expected ranges of&#xA;% two inputs.  The second argument determines that there is only one neuron in&#xA;% the layer. LEARNPN is less sensitive to large variations in input vector size&#xA;% than LEARNP (the default).&#xA;&#xA;net = newp([-40 1;-1 50],1,'hardlim','learnpn');&#xA;&#xA;%%&#xA;% Add the the neuron's initial attempt at classification to the plot.&#xA;%&#xA;% The initial weights are set to zero, so any input gives the same output and&#xA;% the classification line does not even appear on the plot.   Fear not... we are&#xA;% going to train it!&#xA;&#xA;hold on&#xA;linehandle = plotpc(net.IW{1},net.b{1});&#xA;&#xA;%%&#xA;% ADAPT returns a new network object that performs as a better classifier, the&#xA;% network output, and the error.  This loop allows the network to adapt for 3&#xA;% passes, plots the classification line, and continues until the error is zero.&#xA;&#xA;E = 1;&#xA;net.adaptParam.passes = 3;&#xA;while (sse(E))&#xA;   [net,Y,E] = adapt(net,P,T);&#xA;   linehandle = plotpc(net.IW{1},net.b{1},linehandle);&#xA;   drawnow;&#xA;end&#xA;&#xA;%%&#xA;% Note that training with LEARNP took only 3 epochs, while solving the same&#xA;% problem with LEARNPN required 32 epochs.  Thus, LEARNPN does much better job&#xA;% than LEARNP when there are large variations in input vector size.&#xA;&#xA;%%&#xA;% Now SIM can be used to classify any other input vector. For example, classify&#xA;% an input vector of [0.7; 1.2].&#xA;%&#xA;% A plot of this new point with the original training set shows how the network&#xA;% performs.  To distinguish it from the training set, color it red.&#xA;&#xA;p = [0.7; 1.2];&#xA;a = sim(net,p);&#xA;plotpv(p,a);&#xA;circle = findobj(gca,'type','line');&#xA;set(circle,'Color','red');&#xA;&#xA;%%&#xA;% Turn on &#34;hold&#34; so the previous plot is not erased.  Add the training set&#xA;% and the classification line to the plot.&#xA;&#xA;hold on;&#xA;plotpv(P,T);&#xA;plotpc(net.IW{1},net.b{1});&#xA;hold off;&#xA;&#xA;%%&#xA;% Finally, zoom into the area of interest.&#xA;%&#xA;% The perceptron correctly classified our new point (in red) as category &#34;zero&#34;&#xA;% (represented by a circle) and not a &#34;one&#34; (represented by a plus). The&#xA;% perceptron learns properly in much shorter time in spite of the outlier&#xA;% (compare with the &#34;Outlier Input Vectors&#34; demo).&#xA;&#xA;axis([-2 2 -2 2]);&#xA;&#xA;&#xA;"></originalCode>