<!--
This HTML is auto-generated from an m-file.
Your changes will be overwritten.
--><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:x-large">Outlier Input Vectors</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">A 2-input hard limit neuron is trained to classify 5 input vectors into two
categories.  However, because 1 input vector is much larger than all of the
others, training takes a long time.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Copyright 1992-2002 The MathWorks, Inc.
$Revision: 1.18 $  $Date: 2002/03/29 19:36:10 $
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Each of the five column vectors in P defines a 2-element input vectors, and a
row vector T defines the vector's target categories.  Plot these vectors with
PLOTPV.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">P = [-0.5 -0.5 +0.3 -0.1 -40; -0.5 +0.5 -0.5 +1.0 50];
T = [1 1 0 0 1];
plotpv(P,T);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop4_img02.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Note that 4 input vectors have much smaller magnitudes than the fifth vector
in the upper left of the plot.  The perceptron must properly classify the 5
input vectors in P into the two categories defined by T.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">NEWP creates aperceptron.  The first argument specifies the expected ranges
of two inputs.  The second argument determines that there is only one neuron
in the layer.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">net = newp([-40 1;-1 50],1);</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Add the the neuron's initial attempt at classification to the plot.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">The initial weights are set to zero, so any input gives the same output and
the classification line does not even appear on the plot.   Fear not... we are
going to train it!
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">hold on
linehandle = plotpc(net.IW{1},net.b{1});</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop4_img04.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">ADAPT returns a new network object that performs as a better classifier, the
network output, and the error.  This loop allows the network to adapt for 3
passes, plots the classification line, and continues until the error is zero.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">E = 1;
net.adaptParam.passes = 3;
<span style="color:blue">while</span> (sse(E))
   [net,Y,E] = adapt(net,P,T);
   linehandle = plotpc(net.IW{1},net.b{1},linehandle);
   drawnow;
<span style="color:blue">end</span></pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop4_img05.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Note that it took the perceptron many epochs to train.  This a very long time
for such a simple problem.  The reason for the long training time is the
outlier vector.  Despite the long training time, the perceptron still learns
properly and can be used to classify other inputs.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Now SIM can be used to classify any other input vector. For example, classify
an input vector of [0.7; 1.2].
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">A plot of this new point with the original training set shows how the network
performs.  To distinguish it from the training set, color it red.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">p = [0.7; 1.2];
a = sim(net,p);
plotpv(p,a);
circle = findobj(gca,<span style="color:#B20000">'type'</span>,<span style="color:#B20000">'line'</span>);
set(circle,<span style="color:#B20000">'Color'</span>,<span style="color:#B20000">'red'</span>);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop4_img07.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Turn on "hold" so the previous plot is not erased.  Add the training set
and the classification line to the plot.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">hold on;
plotpv(P,T);
plotpc(net.IW{1},net.b{1});
hold off;</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop4_img08.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Finally, zoom into the area of interest.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">The perceptron correctly classified our new point (in red) as category "zero"
(represented by a circle) and not a "one" (represented by a plus).  Despite
the long training time, the perceptron still learns properly.  To see how to
reduce training times associated with outlier vectors, see the "Normalized
Perceptron Rule" demo.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">axis([-2 2 -2 2]);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demop4_img09.gif"><originalCode xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" code="%% Outlier Input Vectors&#xA;% A 2-input hard limit neuron is trained to classify 5 input vectors into two&#xA;% categories.  However, because 1 input vector is much larger than all of the&#xA;% others, training takes a long time.&#xA;%&#xA;% Copyright 1992-2002 The MathWorks, Inc.&#xA;% $Revision: 1.18 $  $Date: 2002/03/29 19:36:10 $&#xA;&#xA;%%&#xA;% Each of the five column vectors in P defines a 2-element input vectors, and a&#xA;% row vector T defines the vector's target categories.  Plot these vectors with&#xA;% PLOTPV.&#xA;&#xA;P = [-0.5 -0.5 +0.3 -0.1 -40; -0.5 +0.5 -0.5 +1.0 50];&#xA;T = [1 1 0 0 1];&#xA;plotpv(P,T);&#xA;&#xA;%%&#xA;% Note that 4 input vectors have much smaller magnitudes than the fifth vector&#xA;% in the upper left of the plot.  The perceptron must properly classify the 5&#xA;% input vectors in P into the two categories defined by T.  &#xA;% &#xA;% NEWP creates aperceptron.  The first argument specifies the expected ranges&#xA;% of two inputs.  The second argument determines that there is only one neuron&#xA;% in the layer.&#xA;&#xA;net = newp([-40 1;-1 50],1);&#xA;&#xA;%%&#xA;% Add the the neuron's initial attempt at classification to the plot.&#xA;%&#xA;% The initial weights are set to zero, so any input gives the same output and&#xA;% the classification line does not even appear on the plot.   Fear not... we are&#xA;% going to train it!&#xA;&#xA;hold on&#xA;linehandle = plotpc(net.IW{1},net.b{1});&#xA;&#xA;%%&#xA;% ADAPT returns a new network object that performs as a better classifier, the&#xA;% network output, and the error.  This loop allows the network to adapt for 3&#xA;% passes, plots the classification line, and continues until the error is zero.&#xA;&#xA;E = 1;&#xA;net.adaptParam.passes = 3;&#xA;while (sse(E))&#xA;   [net,Y,E] = adapt(net,P,T);&#xA;   linehandle = plotpc(net.IW{1},net.b{1},linehandle);&#xA;   drawnow;&#xA;end&#xA;&#xA;%%&#xA;% Note that it took the perceptron many epochs to train.  This a very long time&#xA;% for such a simple problem.  The reason for the long training time is the&#xA;% outlier vector.  Despite the long training time, the perceptron still learns&#xA;% properly and can be used to classify other inputs.&#xA;&#xA;%%&#xA;% Now SIM can be used to classify any other input vector. For example, classify&#xA;% an input vector of [0.7; 1.2].&#xA;%&#xA;% A plot of this new point with the original training set shows how the network&#xA;% performs.  To distinguish it from the training set, color it red.&#xA;&#xA;p = [0.7; 1.2];&#xA;a = sim(net,p);&#xA;plotpv(p,a);&#xA;circle = findobj(gca,'type','line');&#xA;set(circle,'Color','red');&#xA;&#xA;%%&#xA;% Turn on &#34;hold&#34; so the previous plot is not erased.  Add the training set&#xA;% and the classification line to the plot.&#xA;&#xA;hold on;&#xA;plotpv(P,T);&#xA;plotpc(net.IW{1},net.b{1});&#xA;hold off;&#xA;&#xA;%%&#xA;% Finally, zoom into the area of interest.&#xA;%&#xA;% The perceptron correctly classified our new point (in red) as category &#34;zero&#34;&#xA;% (represented by a circle) and not a &#34;one&#34; (represented by a plus).  Despite&#xA;% the long training time, the perceptron still learns properly.  To see how to&#xA;% reduce training times associated with outlier vectors, see the &#34;Normalized&#xA;% Perceptron Rule&#34; demo.&#xA;&#xA;axis([-2 2 -2 2]);&#xA;"></originalCode>