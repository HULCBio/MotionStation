<!--
This HTML is auto-generated from an m-file.
Your changes will be overwritten.
--><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:x-large">Learning Vector Quantization</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">An LVQ network is trained to classify input vectors according to given
targets.
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Copyright 1992-2002 The MathWorks, Inc.
$Revision: 1.14 $  $Date: 2002/03/29 19:36:12 $
</p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Let P be 10 2-element example input vectors and C be the classes these vectors
fall into.  These classes can be transformed into vectors to be used as
targets, T, with IND2VEC.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">P = [-3 -2 -2  0  0  0  0 +2 +2 +3;
      0 +1 -1 +2 +1 -1 -2 +1 -1  0];
C = [1 1 1 2 2 2 2 1 1 1];
T = ind2vec(C);</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Here the data points are plotted.  Red = class 1, Cyan = class 2.  The LVQ
network represents clusters of vectors with hidden neurons, and groups the
clusters with output neurons to form the desired classes.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">colormap(hsv);
plotvec(P,C)
title(<span style="color:#B20000">'Input Vectors'</span>);
xlabel(<span style="color:#B20000">'P(1)'</span>);
ylabel(<span style="color:#B20000">'P(2)'</span>);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demolvq1_img03.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">NEWLVQ creates an LVQ layer and here takes four arguments: Rx2 matrix of min
and max values for R input elements, number of hidden neurons, element vector
of typical class percentages, and learning rate,
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">net = newlvq(minmax(P),4,[.6 .4],0.1);</pre><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">The competitive neuron weight vectors are plotted as follows.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">hold on
W1 = net.IW{1};
plot(W1(1,1),W1(1,2),<span style="color:#B20000">'ow'</span>)
title(<span style="color:#B20000">'Input/Weight Vectors'</span>);
xlabel(<span style="color:#B20000">'P(1), W(1)'</span>);
ylabel(<span style="color:#B20000">'P(2), W(3)'</span>);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demolvq1_img05.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">To train the network, first override the default number of epochs, and then
train the network.  When it is finished, replot the input vectors '+' and the
competitive neurons' weight vectors 'o'. Red = class 1, Cyan = class 2.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">net.trainParam.epochs=150;
net.trainParam.show=Inf;
net=train(net,P,T);

cla;
plotvec(P,C);
hold on;
plotvec(net.IW{1}',vec2ind(net.LW{2}),<span style="color:#B20000">'o'</span>);</pre><img xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" src="demolvq1_img06.gif"><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:#990000; font-weight:bold; font-size:medium; page-break-before: auto;"><a name=""></a></p><p xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">Now use the LVQ network as a classifier, where each neuron corresponds to a
different category.  Present the input vector [0.2; 1].  Red = class 1, Cyan =
class 2.
</p><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="position: relative; left:30px">p = [0.2; 1];
a = vec2ind(sim(net,p))</pre><pre xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" style="color:gray; font-style:italic;">
a =

     2

</pre><originalCode xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd" code="%% Learning Vector Quantization&#xA;% An LVQ network is trained to classify input vectors according to given&#xA;% targets.&#xA;%&#xA;% Copyright 1992-2002 The MathWorks, Inc.&#xA;% $Revision: 1.14 $  $Date: 2002/03/29 19:36:12 $&#xA;&#xA;%%&#xA;% Let P be 10 2-element example input vectors and C be the classes these vectors&#xA;% fall into.  These classes can be transformed into vectors to be used as&#xA;% targets, T, with IND2VEC.&#xA;&#xA;P = [-3 -2 -2  0  0  0  0 +2 +2 +3;&#xA;      0 +1 -1 +2 +1 -1 -2 +1 -1  0];&#xA;C = [1 1 1 2 2 2 2 1 1 1];&#xA;T = ind2vec(C);&#xA;&#xA;%%&#xA;% Here the data points are plotted.  Red = class 1, Cyan = class 2.  The LVQ&#xA;% network represents clusters of vectors with hidden neurons, and groups the&#xA;% clusters with output neurons to form the desired classes.&#xA;&#xA;colormap(hsv);&#xA;plotvec(P,C)&#xA;title('Input Vectors');&#xA;xlabel('P(1)');&#xA;ylabel('P(2)');&#xA;&#xA;%%&#xA;% NEWLVQ creates an LVQ layer and here takes four arguments: Rx2 matrix of min&#xA;% and max values for R input elements, number of hidden neurons, element vector&#xA;% of typical class percentages, and learning rate,&#xA;&#xA;net = newlvq(minmax(P),4,[.6 .4],0.1);&#xA;&#xA;%%&#xA;% The competitive neuron weight vectors are plotted as follows.&#xA;&#xA;hold on&#xA;W1 = net.IW{1};&#xA;plot(W1(1,1),W1(1,2),'ow')&#xA;title('Input/Weight Vectors');&#xA;xlabel('P(1), W(1)');&#xA;ylabel('P(2), W(3)');&#xA;&#xA;%%&#xA;% To train the network, first override the default number of epochs, and then&#xA;% train the network.  When it is finished, replot the input vectors '+' and the&#xA;% competitive neurons' weight vectors 'o'. Red = class 1, Cyan = class 2.&#xA;&#xA;net.trainParam.epochs=150;&#xA;net.trainParam.show=Inf;&#xA;net=train(net,P,T);&#xA;&#xA;cla;&#xA;plotvec(P,C);&#xA;hold on;&#xA;plotvec(net.IW{1}',vec2ind(net.LW{2}),'o');&#xA;&#xA;%%&#xA;% Now use the LVQ network as a classifier, where each neuron corresponds to a&#xA;% different category.  Present the input vector [0.2; 1].  Red = class 1, Cyan =&#xA;% class 2.&#xA;&#xA;p = [0.2; 1];&#xA;a = vec2ind(sim(net,p))&#xA;"></originalCode>