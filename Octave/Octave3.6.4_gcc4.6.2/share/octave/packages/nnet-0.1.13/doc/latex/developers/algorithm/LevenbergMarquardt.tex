\section{Levenberg Marquardt}
This algorithm will be programed with \cite{4}.

\subsection{Sensitivity Matrix}
How does this looks like?\\
\begin{enumerate}
	\item for a 1-1-1 MLP
	\item for a 2-1-1 MLP
	\item for a 1-2-1 MLP
\end{enumerate}

\subsubsection{1-1-1 MLP}
In this case, the MLP holds one input neuron, one hidden neuron and one output neuron. The number of weights needed for this MLP is 4 (2 weights, 2 biases).\\

It needs two sensitivity matrices because the two layers. Each sensitivity matrix will hold 1 element.
This is taken from \cite{4}, example P12.5 page 12-44. Attention, in this example are two data sets used, this is the reason for the 4 elements...!

\subsubsection{2-1-1 MLP}
In this case, the MLP holds two input neurons, one hidden neuron and one output neuron. The number of weights needed for this MLP is 5 (3 weights, 2 biases).\\

It needs also two sensitivity matrices because the two layers. Actually, the input is not only a scalar, it is a vector with 2 elements. Even though, again after \cite{4}. I think the sensitivity matrices will hold only 1 element. So the number of elements will bi proportional to the number of hidden neurons and the number of output neurons.

\subsubsection{1-2-1 MLP}
In this case, the MLP holds one input neuron, two hidden neurons and one output neuron. The number of weights needed for this MLP is 7 (4 weights, 3 biases).\\

It needs also two sensitivity matrices because the two layers. Actually, the input is again only a scalar. 
Now calculating $n_1^1$ will result in a row vector with 2 elements. $n_1^2$ will hold only one element and so we have 3 elements in the sensitivity matrix.\\

We can say, the number of hidden neurons is responsible for the dimension of the sensitivity matrices.
For example, a 4-3-1 MLP with 100 data sets will generate following sensitivity matrix for the first layer:
$\tilde{\textbf{S}}^1 = [\tilde{\textbf{S}}^1_1 | \tilde{\textbf{S}}^1_2 | ... | \tilde{\textbf{S}}^1_{100}]$\\
\noindent $\tilde{\textbf{S}}^1_1$ will hold 3 elements  $\tilde{\textbf{S}}^1_1 = [\tilde{\textbf{S}}^1_{1,1} ~ \tilde{\textbf{S}}^1_{2,1} ~ \tilde{\textbf{S}}^1_{3,1}]^T$;
$\tilde{\textbf{S}}^1_2 = [\tilde{\textbf{S}}^1_{1,2} ~ \tilde{\textbf{S}}^1_{2,2} ~ \tilde{\textbf{S}}^1_{3,2}]^T$ and so on. So matrix will have a size of 3x100 for $\tilde{\textbf{S}}^1_{1}$
and a size of 1x100 for $\tilde{\textbf{S}}^1_{2}$.\\

By the way, the jacobian matrix will be a 100x20 matrix ..



