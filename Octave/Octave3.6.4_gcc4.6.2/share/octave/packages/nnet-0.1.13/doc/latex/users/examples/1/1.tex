\section{Example 1}
You can find this example in the \textit{tests/MLP} directory of each release or from the subversion repository. I will do (more or less) a line by line walkthrough, so after this should be everything clear. I assume that you have some experience with multilayer perceptrons.

\subsection{Introduction}
Our problem can be solved with a monotonically increasing or decreasing surface. An input vector \textbf{p} (with 9 values) should be mapped onto one output value. Because we know that it can be solved with a monotonically increasing or decreasing surface, we can choose a 9-1-1 multi-layer perceptron (short: MLP).
This means an MLP with 9 input neurons, only 1 hidden neuron and with 1 output neuron.

\subsection{Code m-file}
\input{examples/1/MLP9_1_1}

\subsection{Walkthrough}
Till line number 0023 there is realy nothing interesting.\\
On line 0023 \& 0024 data will be loaded. This data matrix contains 13 columns. Column 4, 8 and 12 won't be used (this is because the datas are of a real world problem). Column 13 contains the target values.
So on the lines 0049 till 0051 this will be splittet into the corresponding peaces. A short repetition about the datas: Each line is a data set with 9 input values and one target value. On line 0038 and 0039 the datas are transposed. So we have now in each column one data set.\\

Now let's split the data matrix again in 3 pieces. The biggest part is for training the network. The second part for testing the trained network to be sure it's still possible to generalize with the net. And the third part, and the smallest one, for validate during training. This splitting happens on the lines 0041 till 0061.\\

Line 0063 is the first special command from this toolbox. This command will be used to pre-standardize the input datas. Do it ever! Non linear transfer functions will squash the whole input range to an small second range e.g. the transfer function \textit{tansig} will squash the datas between -1 and +1.\\

On line 0069 the next toolbox command will be used. This command \textit{min\_max} creates a $Rx2$ matrix of the complete input matrix. Don't ask me for what MATLAB(TM) this is using. I couldn't figure out it. One part is the number of input neurons, but for this, the range would not be needed. Who cares ;-)\\

Now it's time to create a structure which holds the informations about the neural network. The command \textbf{newff} can do it for us. See the complete line and actually, please use it only on this way, each other try will fail! This means, you can change the number of input neurons, the number of hidden neurons and the number of output neurons of course. But don't change the train algorithm or the performance function.\\

\textbf{saveMLPStruct} on line 0083 is a command which doesn't exist in MATLAB(TM). This will save the structure with the same informations you can see in MATLAB(TM) if you try to open the net-type.\\

The validation part on line 0086 \& 0087 is important. The naming convention is for MATLAB(TM) compatibility. For validate, you have to define a structure with the name \textbf{VV}. Inside this structure you have to define actually \textbf{VV.P} \& \textbf{VV.T} for validate inputs and validate targets. Bye the way, you have to pre-standardize them like the training input matrix. Use for this the command \textbf{trastd} like on line 0090.\\

\textbf{train} is the next toolbox command and of course one of the most important. Please also use this command like on line 0092. Nothing else will work.\\

The second last step is to standardize again datas. This time the test datas. See line 0096 for this and the last step. Simulate the network. This can be done with the command \textbf{sim}. This will be a critical part if someone else will write a toolbox with this command name!\\

I hope this short walkthrough will help for first steps. In next time, I will try to improve this documentation and of course, the toolbox commands. But time is realy rare.


