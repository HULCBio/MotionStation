\section{analyzing newff}
First, \textit{newff} will be analyzed for a X-X-X mlp. This means, maximum 3 layers, including the input layer. Or in words, one input- one hidden- and one output-layer. The number of neurons are choosable.

Following command will be used, to create a new feed-forward neural network:\\
\noindent MLPnet = newff(mMinMaxElements,[nHiddenNeurons nOutputNeurons],...\newline
\{'tansig','purelin'\},'trainlm','learngdm','mse');\\

newff is the matlab command, mMinMaxElements is a $Rx2$-Matrix with minimum and maximum values of the inputs. $R$ is equal to the number of input neurons. [nHiddenNeurons nOutputNeurons] are the scalar values, to define the number of neurons in the hidden and output layer. One value, for each layer. \{'tansig','purelin'\} are the transfer functions, for each layer. This means, 'tansig' for the hidden layer and 'purelin' for the output layer. 'trainlm' is the training algorithm, in this case, Levenberg-Marquardt. 'learngdm' is the learn algorithm and 'mse' is the performance function, \textbf{m}ean-\textbf{s}quare-\textbf{e}rror.\\
MLPnet will be a structure with following content:

\begin{verbatim}
Neural Network object:

    architecture:

         numInputs: 1
         numLayers: 2
       biasConnect: [1; 1]
      inputConnect: [1; 0]
      layerConnect: [0 0; 1 0]
     outputConnect: [0 1]
     targetConnect: [0 1]

        numOutputs: 1  (read-only)
        numTargets: 1  (read-only)
    numInputDelays: 0  (read-only)
    numLayerDelays: 0  (read-only)

    subobject structures:

            inputs: {1x1 cell} of inputs
            layers: {2x1 cell} of layers
           outputs: {1x2 cell} containing 1 output
           targets: {1x2 cell} containing 1 target
            biases: {2x1 cell} containing 2 biases
      inputWeights: {2x1 cell} containing 1 input weight
      layerWeights: {2x2 cell} containing 1 layer weight

    functions:

          adaptFcn: 'trains'
           initFcn: 'initlay'
        performFcn: 'mse'
          trainFcn: 'trainlm'

    parameters:

        adaptParam: .passes
         initParam: (none)
      performParam: (none)
        trainParam: .epochs, .goal, .max_fail, .mem_reduc, 
                    .min_grad, .mu, .mu_dec, .mu_inc, 
                    .mu_max, .show, .time

    weight and bias values:

                IW: {2x1 cell} containing 1 input weight matrix
                LW: {2x2 cell} containing 1 layer weight matrix
                 b: {2x1 cell} containing 2 bias vectors

    other:

          userdata: (user stuff)
\end{verbatim}
\textit{numInputs: 1}: one input layer\\
\noindent \textit{numLayers: 2}: one hidden and one output layer\\
\noindent \textit{biasConnect: [1; 1]}: unknown till now!!\\
\noindent \textit{inputConnect: [1; 0]}: unknown till now!!\\
\noindent \textit{layerConnect: [0 0; 1 0]}: unknown till now!!\\
\noindent \textit{outputConnect: [0 1]}: unknown till now!!\\
\noindent \textit{targetConnect: [0 1]}: unknown till now!!\\
\noindent \textit{numOutputs: 1  (read-only)}: unknown till now!!\\
\noindent \textit{numTargets: 1  (read-only)}: unknown till now!!\\
\noindent \textit{numInputDelays: 0  (read-only)}: unknown till now!!\\
\noindent \textit{numLayerDelays: 0  (read-only)}: unknown till now!!\\
\noindent \textit{inputs: {1x1 cell} of inputs}: input layer definition\\
Because we have defined only one input layer, you can see the detailed definition with
following command in the matlab prompt:\\
\begin{verbatim}
	MLPnet.inputs{1}

ans = 

       range: [26x2 double]
        size: 26
    userdata: [1x1 struct]
\end{verbatim}
range are the min. and max. values of the inputs. size is the number of input neurons and userdata are user specified inputs...!\\
\noindent \textit{layers: {2x1 cell} of layers}: hidden and output layer definition\\
The dimension of $2x1 cell$ is because we have one hidden and one output layer. So too see the details of the hidden layer definitions, we have to enter:
\begin{verbatim}
K>> MLPnet.layers{1}

ans = 

     dimensions: 2
    distanceFcn: ''
      distances: []
        initFcn: 'initnw'
    netInputFcn: 'netsum'
      positions: [0 1]
           size: 2
    topologyFcn: 'hextop'
    transferFcn: 'tansig'
       userdata: [1x1 struct]
\end{verbatim}
and for the output layer:
\begin{verbatim}
K>> MLPnet.layers{2}

ans = 

     dimensions: 1
    distanceFcn: ''
      distances: []
        initFcn: 'initnw'
    netInputFcn: 'netsum'
      positions: 0
           size: 1
    topologyFcn: 'hextop'
    transferFcn: 'purelin'
       userdata: [1x1 struct]
\end{verbatim}

\noindent \textit{outputs: {1x2 cell} containing 1 output}: output layer definitions\\
\begin{verbatim}
K>> MLPnet.outputs

ans = 

     []    [1x1 struct]
\end{verbatim}
How knows, why this is a $1x2 cell$? The next command will also show the detailed definition! Of course, realy simple.
\begin{verbatim}
K>> MLPnet.outputs{2}

ans = 

        size: 1
    userdata: [1x1 struct]
\end{verbatim} 

\noindent \textit{targets: {1x2 cell} containing 1 target}: unknow till now\\

\noindent \textit{biases: {2x1 cell} containing 2 biases}: detailed definitions, for the biases\\
\begin{verbatim}
K>> MLPnet.biases

ans = 

    [1x1 struct]
    [1x1 struct]

K>> MLPnet.biases{1}

ans = 

       initFcn: ''
         learn: 1
      learnFcn: 'learngdm'
    learnParam: [1x1 struct]
          size: 2
      userdata: [1x1 struct]

K>> MLPnet.biases{2}

ans = 

       initFcn: ''
         learn: 1
      learnFcn: 'learngdm'
    learnParam: [1x1 struct]
          size: 1
      userdata: [1x1 struct]
\end{verbatim}
      inputWeights: {2x1 cell} containing 1 input weight
      layerWeights: {2x2 cell} containing 1 layer weight


\paragraph{weight and bias values:}

\subparagraph{IW:}
\begin{verbatim}
K>> MLPnet.IW

ans = 

    [2x26 double]
               []
\end{verbatim}

\subparagraph{LW:}
\begin{verbatim}
K>> MLPnet.LW

ans = 

              []     []
    [1x2 double]     []
\end{verbatim}

\subparagraph{b:}
\begin{verbatim}
K>> MLPnet.b

ans = 

    [2x1 double]
    [   -0.3908]
\end{verbatim}


\paragraph{net.trainParam:}
Output for the Levenberg-Marquardt train algorithm.
\begin{verbatim}
K>> MLPnet.trainParam

ans = 

       epochs: 100
         goal: 0
     max_fail: 5
    mem_reduc: 1
     min_grad: 1.0000e-010
           mu: 0.0010
       mu_dec: 0.1000
       mu_inc: 10
       mu_max: 1.0000e+010
         show: 25
         time: Inf
\end{verbatim}