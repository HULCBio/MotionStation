\section{Example 1}

This MLP is designed with 2-2-1. This is not a complete example but it will help to understand the
dimensions of all matrices and vectores are used inside the Levenberg-Marquardt algorithm.

\subsection{Data matrices}
The input matrix will be defined like in equation \eqref{equ:mInput} and the output matrix like in 
equation \eqref{equ:mOutput}.

\begin{equation}
  mInput = \left[ \begin{array}{c c c c}
												1 & 2 & 3 & 1   \\
												1	& 1 & 1 & 2 	\\
												1 & 2 & 1 & 2		\\																					
												\end{array}
					\right]
		\label{equ:mInput}
\end{equation}

\begin{equation}
  mOutput = \left[ \begin{array}{c c c c}
												1 & 1.5 & 2 & 3   \\																					
									 \end{array}
					\right]
		\label{equ:mOutput}
\end{equation}




\subsection{Weight matrices}
The first layer matrix will hold 2x3 weights. The second layer matrix will hold 1x2 weights.
The first bias holds 3x1 weights and the second holds only a scalar element.

\subsection{Sensitivity Matrices}
This part is right now not so clear in my mind. What is the dimension of these two matrices?
The first layer sensitivity matrix should be about 2x71. Number of hidden neurons in the rows and number of train data sets in the columns.\\

In the actual version, the dimension is about 71x71 .. so it seems to have a mistake inside the algorithm :-(

